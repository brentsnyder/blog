{
  
    
        "post0": {
            "title": "Title",
            "content": ". . title: &quot;Predict Zillow home prices ranges in Austin, TX using XGBoost&quot; description: &quot;In this post I will break down a few strategies I used in my approach&quot; layout: post toc: false comments: true image: images/some_folder/your_image.png hide: false search_exclude: true . categories: [python, modeling] . Predict Zillow home prices ranges in Austin, TX using XGBoost . In season 1 episode 11 of SLICED, a Kaggle hosted competitive data science streaming show, contestants were given the task of predicting Zillow home price ranges. . I competing alongside the contestants in the 2-hour window and actually did pretty good. The final private leaderboard had me at 2nd place out of 23 contestants. It&#39;s amazing to see an active and engaged data science community. Excited to have the opportunity to compete! . In this post I will break down a few strategies I used in my approach and provide commentary on what I would have done beyond 2 hours. . . Import packages and set up the train/test data . The train data has 10,000 homes with no missing values. Mostly numerical features but a few categorical features that may be useful. . df_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 uid 10000 non-null int64 1 city 10000 non-null object 2 description 10000 non-null object 3 homeType 10000 non-null object 4 latitude 10000 non-null float64 5 longitude 10000 non-null float64 6 garageSpaces 10000 non-null int64 7 hasSpa 10000 non-null bool 8 yearBuilt 10000 non-null int64 9 numOfPatioAndPorchFeatures 10000 non-null int64 10 lotSizeSqFt 10000 non-null float64 11 avgSchoolRating 10000 non-null float64 12 MedianStudentsPerTeacher 10000 non-null int64 13 numOfBathrooms 10000 non-null float64 14 numOfBedrooms 10000 non-null int64 15 priceRange 10000 non-null object dtypes: bool(1), float64(5), int64(6), object(4) memory usage: 1.2+ MB . We will be predicting the &#39;priceRange&#39; variable. This is a multi-class problem. . Thankfully, there does not appear to be a severe class imbalance. There are fewer very expensive homes in the $650,000+ range and fewer lower priced homes in the $0-$250,000 range. . df_train[&#39;priceRange&#39;].value_counts() . 250000-350000 2356 350000-450000 2301 450000-650000 2275 650000+ 1819 0-250000 1249 Name: priceRange, dtype: int64 . Exploratory Analysis . What is an important predictor of price? Location, location, location . Austin prices are more expensive in the inner-most part of the city and are typically cheaper in the south and east sides of the city. Latitude and longitude are expected to be great predictors of price. . price_dict = {&#39;650000+&#39;:600000, &#39;350000-450000&#39;:400000, &#39;0-250000&#39;:200000, &#39;450000-650000&#39;:500000, &#39;250000-350000&#39;:300000} df_train[&#39;price&#39;] = df_train[&#39;priceRange&#39;].map(price_dict) . sns.scatterplot(data=df_train, x=&quot;longitude&quot;, y=&quot;latitude&quot;, hue=&#39;price&#39;, palette=&#39;Blues&#39;) plt.legend(bbox_to_anchor=(1.05,1), loc=2) plt.title(&#39;Austin prices differ based on location&#39;, loc=&#39;left&#39;, fontdict = {&#39;fontsize&#39; : 16}); . The number of bedrooms and bathrooms may be useful for predicting price range. The boxplots below show that the number of bathrooms may be more helpful than number of bedrooms. . plt.figure(figsize=(10,5)) sns.boxplot(data=df_train, x=&quot;priceRange&quot;, y=&quot;numOfBathrooms&quot;, order=[&#39;0-250000&#39;,&#39;250000-350000&#39;, &#39;350000-450000&#39;, &#39;450000-650000&#39;, &#39;650000+&#39;]) plt.title(&#39;Median # of bathrooms is 2 for home prices less than $650K&#39;, loc=&#39;left&#39;, fontdict = {&#39;fontsize&#39; : 16}); . plt.figure(figsize=(10,5)) sns.boxplot(data=df_train, x=&quot;priceRange&quot;, y=&quot;numOfBedrooms&quot;, order=[&#39;0-250000&#39;,&#39;250000-350000&#39;, &#39;350000-450000&#39;, &#39;450000-650000&#39;, &#39;650000+&#39;]) plt.title(&#39;Number of Bedrooms does not appear to differ by price range&#39;, loc=&#39;left&#39;, fontdict = {&#39;fontsize&#39; : 16}); . There are 3 categorical features besides the target. There are great opportunities for creating text features on the description feature in the future. . df_train.columns[df_train.dtypes == &#39;object&#39;] . Index([&#39;city&#39;, &#39;description&#39;, &#39;homeType&#39;, &#39;priceRange&#39;], dtype=&#39;object&#39;) . I&#39;ve looked at home price data in the past and I&#39;ve found features that are similar to the home type are significant predictors. . Let&#39;s quickly dummy encode the homeType feature for use in modeling and prepare features for modeling. . df_test[&#39;priceRange&#39;] = -1 # join the train and test dataframes df = pd.concat([df_train, df_test]) # dummy encoding df = pd.get_dummies(df, columns = [&#39;homeType&#39;]) df_train = df.loc[df[&#39;priceRange&#39;]!=-1,:] df_test = df.loc[df[&#39;priceRange&#39;] == -1,:] . Modeling . num_feats = [ &#39;latitude&#39;, &#39;longitude&#39;, &#39;yearBuilt&#39;, &#39;garageSpaces&#39;, &#39;numOfPatioAndPorchFeatures&#39;, &#39;lotSizeSqFt&#39;, &#39;avgSchoolRating&#39;, &#39;MedianStudentsPerTeacher&#39;, &#39;numOfBathrooms&#39;, &#39;numOfBedrooms&#39;, &#39;hasSpa&#39; ] # only considering homeType dummy features. I might try description later. cat_feats = [f for f in df_train.columns if (f.find(&#39;homeType&#39;)!=-1)] # define the target we are going to predict target = &#39;priceRange&#39; . First, let&#39;s use KFold cross validation to build a baseline XGBoost model using default parameters to measure our Multiclass log loss. . folds = 10 kf = KFold(folds) df_preds = pd.DataFrame() for train_idx, test_idx in tqdm(kf.split(df_train), total=folds): train_data = df_train.iloc[train_idx] test_data = df_train.iloc[test_idx].copy() xgb_mod = xgb.XGBClassifier(objective = &#39;multi:softprob&#39;, n_jobs=-1, num_class = 5, learning_rate=0.01, n_estimators=1000, random_state=20210810 ) xgb_mod.fit(train_data.loc[:, num_feats+cat_feats], train_data[target]) test_data[&#39;0-250000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,0], index=test_data.index) test_data[&#39;250000-350000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,1], index=test_data.index) test_data[&#39;350000-450000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,2], index=test_data.index) test_data[&#39;450000-650000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,3], index=test_data.index) test_data[&#39;650000+&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,4], index=test_data.index) df_preds = df_preds.append(test_data) . log_loss(df_preds[&#39;priceRange&#39;], df_preds.loc[:,[&#39;0-250000&#39;, &#39;250000-350000&#39;, &#39;350000-450000&#39;, &#39;450000-650000&#39;, &#39;650000+&#39;]]) . 1.0048688145888969 . That is not a great CV log loss but it provides a baseline for comparison after we make improvements. . Let&#39;s get serious, tune parameters, and then measure the performance improvement (hopefully). . Tuning parameters can be done in a few ways: . Manually | Greedy Grid Search | Randomized Grid Search | Bayesian optimization | ... many others | Analytics Vidhya has an excellent article on different parameter tuning strategies and implementations that I highly recommend. . folds = 10 kf = KFold(folds) df_preds = pd.DataFrame() for train_idx, test_idx in tqdm(kf.split(df_train), total=folds): train_data = df_train.iloc[train_idx] test_data = df_train.iloc[test_idx].copy() xgb_mod = xgb.XGBClassifier(objective = &#39;multi:softprob&#39;, n_jobs=-1, # hyperparameters tuned using randomized grid search below num_class = 5, subsample= 0.8, n_estimators = 1200, min_child_weight = 5, max_depth = 7, learning_rate = 0.015, gamma = 1, colsample_bytree = 0.6, random_state=20210810 ) xgb_mod.fit(train_data.loc[:, num_feats+cat_feats], train_data[target]) test_data[&#39;0-250000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,0], index=test_data.index) test_data[&#39;250000-350000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,1], index=test_data.index) test_data[&#39;350000-450000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,2], index=test_data.index) test_data[&#39;450000-650000&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,3], index=test_data.index) test_data[&#39;650000+&#39;] = pd.DataFrame(xgb_mod.predict_proba(test_data.loc[:, num_feats+cat_feats])[:,4], index=test_data.index) df_preds = df_preds.append(test_data) . log_loss(df_preds[&#39;priceRange&#39;], df_preds.loc[:,[&#39;0-250000&#39;, &#39;250000-350000&#39;, &#39;350000-450000&#39;, &#39;450000-650000&#39;, &#39;650000+&#39;]]) . 0.8938031872542342 . A log loss of 0.89 is an 11% improvement over the baseline model of 1.00. Awesome to see the value of tuning parameters and the impact on log loss. . XGBoost feature importance plots provide insight that x, y, and z are important in the model and a,b,c are not as important . importances = xgb_mod.feature_importances_ idxs = np.argsort(importances) plt.figure(figsize=(10,6)) plt.title(&#39;Feature Importances&#39;) plt.barh(range(len(idxs)), importances[idxs], align=&#39;center&#39;) plt.yticks(range(len(idxs)), [train_data.loc[:, num_feats+cat_feats].columns[i] for i in idxs]) plt.xlabel(&#39;Feature Importance&#39;) plt.show() . Wrap up . The 2 hour time limit was a great forcing function to focus on the most necessary activities to deliver a strong model. . In future iterations I would test: . Text analysis on the description feature to extract additional information. | Explore average school rating and median students per teacher. | Possibly bring in external datasets to factor in additional socioeconomic factors. | Additional location based features. | Try additional models (LightGBM, catboost, etc.). |",
            "url": "https://brentsnyder.github.io/blog/2021/08/14/_08_14_Predict_Zillow_Home_Prices.html",
            "relUrl": "/2021/08/14/_08_14_Predict_Zillow_Home_Prices.html",
            "date": " • Aug 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://brentsnyder.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://brentsnyder.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://brentsnyder.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://brentsnyder.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}